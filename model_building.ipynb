{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "677989f0",
      "metadata": {
        "id": "677989f0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "523111a0",
      "metadata": {
        "id": "523111a0"
      },
      "outputs": [],
      "source": [
        "with open('data.txt') as f:\n",
        "    corpus = f.read()\n",
        "corpus = sent_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0796cb01",
      "metadata": {
        "id": "0796cb01"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e1f9cefd",
      "metadata": {
        "id": "e1f9cefd"
      },
      "outputs": [],
      "source": [
        "tokenizer.fit_on_texts(corpus)\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "# Pad shorter sequences\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "# Assemble training data\n",
        "X_train, labels = input_sequences[:,:-1], input_sequences[:,-1]\n",
        "y_train = to_categorical(labels, num_classes=len(tokenizer.word_index) + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f2fd3c20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2fd3c20",
        "outputId": "851e6376-5d44-4149-ee56-a2d00db89b9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "304/304 [==============================] - 11s 22ms/step - loss: 7.1092 - accuracy: 0.0430\n",
            "Epoch 2/10\n",
            "304/304 [==============================] - 7s 23ms/step - loss: 6.2200 - accuracy: 0.0785\n",
            "Epoch 3/10\n",
            "304/304 [==============================] - 7s 22ms/step - loss: 5.2192 - accuracy: 0.1261\n",
            "Epoch 4/10\n",
            "304/304 [==============================] - 6s 21ms/step - loss: 4.0443 - accuracy: 0.2196\n",
            "Epoch 5/10\n",
            "304/304 [==============================] - 7s 21ms/step - loss: 2.9416 - accuracy: 0.3625\n",
            "Epoch 6/10\n",
            "304/304 [==============================] - 7s 21ms/step - loss: 2.1044 - accuracy: 0.5116\n",
            "Epoch 7/10\n",
            "304/304 [==============================] - 6s 21ms/step - loss: 1.5452 - accuracy: 0.6263\n",
            "Epoch 8/10\n",
            "304/304 [==============================] - 6s 21ms/step - loss: 1.2002 - accuracy: 0.7013\n",
            "Epoch 9/10\n",
            "304/304 [==============================] - 6s 21ms/step - loss: 0.9785 - accuracy: 0.7525\n",
            "Epoch 10/10\n",
            "304/304 [==============================] - 7s 21ms/step - loss: 0.8687 - accuracy: 0.7778\n"
          ]
        }
      ],
      "source": [
        "total_words = len(tokenizer.word_index) + 1\n",
        "print(total_words)\n",
        "# Create model and layers\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150)))\n",
        "model.add(Dense(total_words, activation=\"softmax\"))\n",
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), metrics=['accuracy'])\n",
        "# Fit model to training data\n",
        "fitting = model.fit(X_train, y_train, epochs=10, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_text = 'BERKELEY, Calif. ‚Äì '\n",
        "for _ in range(400):\n",
        "    token_list = tokenizer.texts_to_sequences([full_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    full_text += \" \" + output_word"
      ],
      "metadata": {
        "id": "uoUVT8EDWRvX"
      },
      "id": "uoUVT8EDWRvX",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "qaR1aW17XmRX",
        "outputId": "d85b68e4-6fb3-49d7-aaf5-24b3ab38dddd"
      },
      "id": "qaR1aW17XmRX",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BERKELEY, Calif. ‚Äì  after years of begging for disability accommodations campus super senior ash jensen was miraculously cured of their chronic illness when their philosophy professor proclaimed that they were a failed econ quiz sitting instagram post for a future on sproul plaza god admitted that he was last member provided the namesake for the courtyard and lived as an object of study in the hearst museum during his final years post by berkeley students lining are an in person fall 2021 semester at a quinoa salad appetizer and the little electric shock that is always she‚Äôs to be an earthquake didn‚Äôt helpless they be sustained when their role model again prospective while be honest i‚Äôve to suck is oski‚Äôs struggles of their chronic pain who swiped up with the founders of theranos solyndra cambridge analytica or some other innovative ground breaking company a sarcophagus which is just had a bad completely true admitted student the only dick we get to suck is oski‚Äôs earthquake just had sex with your mom are we sure that 5 inches is average because that seems with tables staffed by overworked students lining its contours little flyers for events of coffee and spilling some on reddit proves it a look ‚Äù a minor dix continued on his professors project in america overworked of a half bull through a trancelike thousand yard stare was a member of their while sprinting by the golden bear‚Äôs golden load tumbling down a esophagus and is it‚Äôs be able with friends ‚Äì ‚Äù fellow dke josh and slap professor claire leighy project usc on the college and because on an erg machine i got the next i made lifelong friends rerun with thebedeau include james ‚Äù the university has grown to talk really over to myself because with your health as well over what the holiday is truly all about the friends we made along the way‚Ä¶ üôÇ no wait that‚Äôs stupid isn‚Äôt the cause of a senior who is lil big penis has turned to the north american public and we don‚Äôt judge it didn‚Äôt care about like for an unlikely solution of coffee and a sense of humanity to the kind of girl who always seemed to be left with and clothing distribution at sucking dick ‚Äù ugba 69 explained ann harrison dean of the haas groundhog containers ‚Äù said fielding golden lose the life century james help also noticed'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MB87wVkgXu3x"
      },
      "id": "MB87wVkgXu3x",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}